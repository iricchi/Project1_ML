{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# add path\n",
    "import sys\n",
    "my_path = r'D:\\Documents\\etudes\\epfl\\MA1\\cours\\MachineLearning\\Project1' \n",
    "path_data_test = r'C:\\Users\\Tom\\Desktop'\n",
    "sys.path.insert(0,my_path + r'\\code\\SUBMISSION')\n",
    "\n",
    "# imports\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from costs import *\n",
    "from optimize_hyperparams import *\n",
    "from cross_validation import *\n",
    "from step_wise import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data \n",
    "\n",
    "# load raw data\n",
    "y_raw, input_data_raw, ids = load_csv_data(my_path + r'\\data\\train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-999 are replaced by the mean value of the feature\n"
     ]
    }
   ],
   "source": [
    "from outliers import handle_outliers\n",
    "\n",
    "# handle outliers\n",
    "X_raw, y = handle_outliers(input_data_raw, y_raw, -999, 'mean')\n",
    "\n",
    "# set y in {0,1} instead of {-1,1}\n",
    "y[np.where(y==-1)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get feature names \n",
    "all_features_raw = list(np.genfromtxt(my_path + r'/data/train.csv', delimiter=\",\", dtype=str, max_rows = 1)[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Features have been set to the power(s): [1]\n",
      "16 Features of the momentum have been added\n",
      "4 logarithmic features have been added.\n",
      "(250000, 50)\n"
     ]
    }
   ],
   "source": [
    "from extend_features import extend_features\n",
    "\n",
    "# feature degree\n",
    "degree = 1\n",
    "\n",
    "# extend feature set\n",
    "all_candidates, features = extend_features(X_raw, all_features_raw, degree, is_add_log = True)\n",
    "print(all_candidates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (time demanding - see bellow to only load the trained weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection (best feature indices from the step wise with logistic regression)\n",
    "indx = [1, 13, 4, 46, 0, 11, 44, 43, 7, 2, 16, 48, 10, 6, 49, 22, 45, 12, 19, 23, 32, 24, 17, 14, 39, 42, 30, 31, 47, 38, 20]\n",
    "\n",
    "# thresholding to lower the number of feature\n",
    "indx = indx[:17]\n",
    "\n",
    "# training set\n",
    "X = all_candidates[:, indx]\n",
    "\n",
    "# optimal degree obtained from degree optimization that uses cross validation with different degrees in [1,10]  \n",
    "degree_opt = 5\n",
    "\n",
    "# build polynomial basis function\n",
    "phi = build_poly(X, degree_opt)\n",
    "        \n",
    "# standardization\n",
    "phi_tmp,_,_ =  standardize(phi[:,1:]) \n",
    "phi[:,1:] = phi_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters (tuned manually to insure convergence)\n",
    "gamma = 1e-5\n",
    "threshold = 1e-3\n",
    "max_iters = 10000\n",
    "initial_w = np.zeros(phi.shape[1])\n",
    "\n",
    "# logistic regression\n",
    "w_tot, loss_tot = logistic_regression(y, phi, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the optimal trained weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_opt = np.load('w_opt_lr.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data \n",
    "\n",
    "# load testing set\n",
    "y_raw_te, input_data_raw_te, ids_te = load_csv_data(path_data_test + r'\\test.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names \n",
    "all_features_raw = list(np.genfromtxt(path_data_test + r'/test.csv', delimiter=\",\", dtype=str, max_rows = 1)[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-999 are replaced by the mean value of the feature\n"
     ]
    }
   ],
   "source": [
    "# handle outliers\n",
    "input_data_raw_te, _ = handle_outliers(input_data_raw_te, y_raw_te, -999, 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Features have been set to the power(s): [1]\n",
      "16 Features of the momentum have been added\n",
      "4 logarithmic features have been added.\n"
     ]
    }
   ],
   "source": [
    "from extend_features import extend_features\n",
    "\n",
    "# feature degree\n",
    "degree = 1\n",
    "\n",
    "# extend feature set\n",
    "X_te, _ = extend_features(input_data_raw_te, all_features_raw, degree, is_add_log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 50)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'indx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-08c1c63e7f0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# feature selection (same as for the training set: see above)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'indx' is not defined"
     ]
    }
   ],
   "source": [
    "# feature selection (best feature indices from the step wise with logistic regression)\n",
    "indx = [1, 13, 4, 46, 0, 11, 44, 43, 7, 2, 16, 48, 10, 6, 49, 22, 45, 12, 19, 23, 32, 24, 17, 14, 39, 42, 30, 31, 47, 38, 20]\n",
    "\n",
    "# thresholding to lower the number of feature\n",
    "indx = indx[:17]\n",
    "\n",
    "# feature selection (same as for the training set: see above)\n",
    "print(X_te.shape)\n",
    "X_te = X_te[:, indx]\n",
    "print(X_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build polynomial basis function\n",
    "degree_opt = 5\n",
    "phi = build_poly(X_te, degree_opt)\n",
    "        \n",
    "# standardization\n",
    "phi_tmp,_,_ =  standardize(phi[:,1:]) \n",
    "phi[:,1:] = phi_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict labels in testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import predict_labels_log\n",
    "\n",
    "# predict labels\n",
    "y_pred = predict_labels_log(w_opt,phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a submission csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace 0 in labels per -1\n",
    "y_pred[np.where(y_pred==0)] = -1\n",
    "\n",
    "# create the csv file\n",
    "create_csv_submission(ids_te, y_pred, \"submission_30_10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
