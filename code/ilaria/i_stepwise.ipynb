{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features selection with STEPWISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "my_path = r'/home/ilaria/Scrivania/Machine_Learning/Project_1/Project1_ML'\n",
    "sys.path.insert(0,my_path + r'/code/COMMON')\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import load_csv_data, predict_labels \n",
    "from implementations import *\n",
    "from outliers import handle_outliers\n",
    "from labels import idx_2labels\n",
    "from standard import standardize\n",
    "from costs import compute_loglikelihood_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(my_path + r'/data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-999 are replaced by the mean value of the feature\n"
     ]
    }
   ],
   "source": [
    "input_data, Y = handle_outliers(input_data,yb,-999,'mean') # substiution with mean because the standardization\n",
    "                                                           #can be affected, otherwise we should delete the whole row\n",
    "ind_back, ind_sig = idx_2labels(Y, [-1,1])\n",
    "Y[ind_back] = 0\n",
    "\n",
    "input_data, mean_X, std_X = standardize(input_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subdived the X features space in single features\n",
    "all_features = np.genfromtxt(my_path + r'/data/train.csv', delimiter=\",\", dtype=str, max_rows = 1)[2:]\n",
    "# converting array in list in order to simplify the adding of features\n",
    "all_features = list(all_features)\n",
    "\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R^2  as stopping criteria : R2 with error , R2 of Tjur or R2 of McFadden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only way to use the stepwise is using R2 of Tjur or McFadden because of the binary values of the indipendent variable, but the error was also used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def results_r2_stepwise(list_r2_adj,indices_features):\n",
    "    print(\"R2 asjusted values:\")\n",
    "    \n",
    "    for i in range(len(list_r2_adj)):\n",
    "        print(list_r2_adj[i])\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Number of features chosen:\", len(indices_features))\n",
    "    print(\"\\n\")\n",
    "    print(\"Indices of features chosen: \", indices_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the error before binarization (R2 with loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltaeta_jet_jet (index : 4 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_pt (index : 16 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_ratio_lep_tau (index : 10 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_lep_eta_centrality (index : 12 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_pt (index : 23 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_pt (index : 26 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_num (index : 22 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_sumet (index : 21 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_MMC (index : 0 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_prodeta_jet_jet (index : 6 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_h (index : 3 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        #y = predict_labels(ws,X)   #***** NO USE OF PREDICTION\n",
    "        SSE = loss\n",
    "        SST = np.sum((Y- Y.mean())**2)\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.999998246996\n",
      "0.999998316254\n",
      "0.999998366571\n",
      "0.999998394757\n",
      "0.99999840969\n",
      "0.999998424917\n",
      "0.99999844367\n",
      "0.999998459265\n",
      "0.999998468735\n",
      "0.999998474896\n",
      "0.99999847968\n",
      "0.999998482105\n",
      "0.999998484408\n",
      "0.999998486981\n",
      "0.99999848737\n",
      "0.999998488309\n",
      "0.999998488373\n",
      "0.999998488384\n",
      "0.99999848839\n",
      "0.999998488392\n",
      "0.999998488393\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 21\n",
      "\n",
      "\n",
      "Indices of features chosen:  [1, 13, 4, 11, 7, 2, 16, 10, 19, 12, 23, 8, 5, 26, 22, 21, 0, 18, 6, 3, 28]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_h (index : 3 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_pt (index : 26 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_all_pt (index : 29 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_eta (index : 27 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_eta (index : 24 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X) \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.346306088284\n",
      "0.364393069363\n",
      "0.380616523588\n",
      "0.389912517644\n",
      "0.394024256458\n",
      "0.396117256195\n",
      "0.397938510559\n",
      "0.398364531728\n",
      "0.398434460516\n",
      "0.398560902406\n",
      "0.398597637228\n",
      "0.398607879751\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 12\n",
      "\n",
      "\n",
      "Indices of features chosen:  [5, 13, 3, 26, 2, 19, 7, 29, 8, 28, 27, 24]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_sum_pt (index : 9 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_eta (index : 24 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = compute_loglikelihood_reg(y,X,w0)#np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = compute_loglikelihood_reg(y,X,ws) #np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.127199992503\n",
      "0.161949175423\n",
      "0.162775033532\n",
      "0.163218892521\n",
      "0.163296584222\n",
      "0.163326074239\n",
      "0.163331726027\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 7\n",
      "\n",
      "\n",
      "Indices of features chosen:  [1, 11, 9, 2, 18, 28, 24]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8376553003945949"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglike/loglike0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure that cross validation can be used we don't have to estimate any hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 with likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from split_data import split_data\n",
    "\n",
    "sys.path.insert(0,my_path + r'/code/ilaria')\n",
    "from i_cross_validation_methods import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_sumet (index : 21 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_phi (index : 20 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        # CROSS-VALIDATION\n",
    "        \n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_ls(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.126787772738\n",
      "0.16193544931\n",
      "0.162749175727\n",
      "0.162788157025\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 4\n",
      "\n",
      "\n",
      "Indices of features chosen:  [1, 11, 21, 20]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used only lambda as hyperparameter because I am not building a polynomial model. But we can add different features transformation (square or log or power), so maybe we can test the polynomial after the best features are selected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### No cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set lambda\n",
    "lambda_ = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the loss from ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltaeta_jet_jet (index : 4 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_pt (index : 16 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_ratio_lep_tau (index : 10 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_lep_eta_centrality (index : 12 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_pt (index : 23 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_pt (index : 26 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_num (index : 22 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_sumet (index : 21 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # start with lambda = 0  \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        SSE = loss\n",
    "        SST = np.sum((Y- Y.mean())**2)   # it has no sense\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.999998221204\n",
      "0.999998289786\n",
      "0.999998339379\n",
      "0.999998368275\n",
      "0.999998382584\n",
      "0.999998395596\n",
      "0.999998410504\n",
      "0.99999842304\n",
      "0.99999843101\n",
      "0.999998437685\n",
      "0.99999844106\n",
      "0.999998443401\n",
      "0.999998445785\n",
      "0.999998448276\n",
      "0.999998448659\n",
      "0.999998449243\n",
      "0.999998449256\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 17\n",
      "\n",
      "\n",
      "Indices of features chosen:  [1, 13, 4, 11, 7, 2, 16, 10, 19, 12, 23, 8, 5, 26, 22, 21, 18]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "No signal detected\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_h (index : 3 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_pt (index : 26 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_all_pt (index : 29 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_eta (index : 27 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_eta (index : 24 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.346306074432\n",
      "0.364393055795\n",
      "0.380616512406\n",
      "0.389912503999\n",
      "0.394024242078\n",
      "0.396117236788\n",
      "0.397938478897\n",
      "0.398364502597\n",
      "0.398434431665\n",
      "0.398560873545\n",
      "0.398597608378\n",
      "0.398607850898\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 12\n",
      "\n",
      "\n",
      "Indices of features chosen:  [5, 13, 3, 26, 2, 19, 7, 29, 8, 28, 27, 24]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltaeta_jet_jet (index : 4 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_lep_eta_centrality (index : 12 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_h (index : 3 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_pt (index : 16 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_pt (index : 23 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_num (index : 22 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_sumet (index : 21 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_MMC (index : 0 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_phi (index : 20 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_eta (index : 17 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_eta (index : 27 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_eta (index : 24 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_eta (index : 14 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_phi (index : 15 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_phi (index : 25 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # lambda set to 0 \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = compute_loglikelihood_reg(y,X,w0)#np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = compute_loglikelihood_reg(y,X,ws) #np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.144854047951\n",
      "0.245180817163\n",
      "0.352173598228\n",
      "0.41170504767\n",
      "0.424708632929\n",
      "0.432288557592\n",
      "0.443235724243\n",
      "0.473466109895\n",
      "0.477541148029\n",
      "0.483139657023\n",
      "0.490491175076\n",
      "0.492657971875\n",
      "0.494326119857\n",
      "0.495663494828\n",
      "0.496289473241\n",
      "0.496334681561\n",
      "0.496370986565\n",
      "0.496388422301\n",
      "0.49640087404\n",
      "0.496411964442\n",
      "0.496414840522\n",
      "0.496418199262\n",
      "0.496421128045\n",
      "0.496423325284\n",
      "0.496423537179\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 25\n",
      "\n",
      "\n",
      "Indices of features chosen:  [13, 1, 4, 11, 12, 19, 3, 16, 2, 7, 23, 8, 22, 21, 5, 18, 0, 20, 17, 28, 27, 24, 14, 15, 25]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With cross validation (it's needed in order to choose the best hyperparameter lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the loss of ridge regression as the error of R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # start with lambda = 0  \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_rr(Y,X)\n",
    "        \n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        loss = np.min(loss_te_tot)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        SSE = loss\n",
    "        SST = np.sum((Y- Y.mean())**2)\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 0\n",
      "\n",
      "\n",
      "Indices of features chosen:  []\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No signal detected\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-55671c8627d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#CROSS VALIDATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mw_tr_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_rr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_tr_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te_tot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilaria/Scrivania/Machine_Learning/Project_1/Project1_ML/code/ilaria/i_cross_validation_methods.py\u001b[0m in \u001b[0;36mcross_validation_rr\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# compute losses for the k'th fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mw_tr_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse_tr_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# store losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilaria/Scrivania/Machine_Learning/Project_1/Project1_ML/code/ilaria/i_cross_validation_methods.py\u001b[0m in \u001b[0;36mcross_validation_r\u001b[0;34m(y, x, k_indices, k, lambda_)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0my_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mw_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilaria/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munion1d\u001b[0;34m(ar1, ar2)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilaria/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_rr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.692583907083\n",
      "0.728802362346\n",
      "0.761239966883\n",
      "0.779850696222\n",
      "0.788063711181\n",
      "0.792264545618\n",
      "0.795889920268\n",
      "0.796756354309\n",
      "0.7969376439\n",
      "0.797170043648\n",
      "0.797259734087\n",
      "0.797271889943\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 12\n",
      "\n",
      "\n",
      "Indices of features chosen:  [5, 13, 3, 26, 2, 19, 7, 29, 8, 28, 27, 24]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltaeta_jet_jet (index : 4 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_lep_eta_centrality (index : 12 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_h (index : 3 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_pt (index : 16 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_pt (index : 23 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_num (index : 22 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_sumet (index : 21 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_MMC (index : 0 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_phi (index : 20 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_eta (index : 17 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_eta (index : 27 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_eta (index : 24 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_phi (index : 15 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_eta (index : 14 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_phi (index : 25 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # lambda set to 0 \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_rr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.144857765242\n",
      "0.245184572292\n",
      "0.352177585943\n",
      "0.411708293799\n",
      "0.424710904491\n",
      "0.432292091945\n",
      "0.443241438144\n",
      "0.47347719231\n",
      "0.477550694143\n",
      "0.483149352784\n",
      "0.4905044717\n",
      "0.4926683267\n",
      "0.494334962769\n",
      "0.495673376908\n",
      "0.496299873075\n",
      "0.496344537618\n",
      "0.496380190427\n",
      "0.496397587194\n",
      "0.496409721352\n",
      "0.496420779729\n",
      "0.496423639692\n",
      "0.49642657291\n",
      "0.496428791606\n",
      "0.496430850605\n",
      "0.496431354733\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 25\n",
      "\n",
      "\n",
      "Indices of features chosen:  [13, 1, 4, 11, 12, 19, 3, 16, 2, 7, 23, 8, 22, 21, 5, 18, 0, 20, 17, 28, 27, 24, 15, 14, 25]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No cross validation (hyperparameter fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "max_iters = 100\n",
    "threshold = 1e-8\n",
    "gamma = 0\n",
    "method = 'gd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used only this method because it's the most reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (99/99): loss logLikelihood=173286.79513998624\n",
      "Logistic Regression (99/99): loss logLikelihood=256647.03707286506\n",
      "Logistic Regression (99/99): loss logLikelihood=305370.71369767754\n",
      "Logistic Regression (99/99): loss logLikelihood=260236.64223629967\n",
      "Logistic Regression (99/99): loss logLikelihood=237425.33824442254\n",
      "Logistic Regression (99/99): loss logLikelihood=241186.74182750197\n",
      "Logistic Regression (99/99): loss logLikelihood=235971.648882099\n",
      "Logistic Regression (99/99): loss logLikelihood=282777.996637951\n",
      "Logistic Regression (99/99): loss logLikelihood=265413.2507708407\n",
      "Logistic Regression (99/99): loss logLikelihood=260872.64876395685\n",
      "Logistic Regression (99/99): loss logLikelihood=242101.6740928538\n",
      "Logistic Regression (99/99): loss logLikelihood=283683.4708764883\n",
      "Logistic Regression (99/99): loss logLikelihood=234027.03455364294\n",
      "Logistic Regression (99/99): loss logLikelihood=244874.22620079634\n",
      "Logistic Regression (99/99): loss logLikelihood=230404.8715407175\n",
      "Logistic Regression (99/99): loss logLikelihood=266435.1797576615\n",
      "Logistic Regression (99/99): loss logLikelihood=266999.9070403106\n",
      "Logistic Regression (99/99): loss logLikelihood=262792.28190689\n",
      "Logistic Regression (99/99): loss logLikelihood=266180.9756148977\n",
      "Logistic Regression (99/99): loss logLikelihood=266160.72087198746\n",
      "Logistic Regression (99/99): loss logLikelihood=256452.2140939233\n",
      "Logistic Regression (99/99): loss logLikelihood=265607.16987403395\n",
      "Logistic Regression (99/99): loss logLikelihood=245375.6370906721\n",
      "Logistic Regression (99/99): loss logLikelihood=248823.21543542342\n",
      "Logistic Regression (99/99): loss logLikelihood=254406.37593137246\n",
      "Logistic Regression (99/99): loss logLikelihood=265660.04966660053\n",
      "Logistic Regression (99/99): loss logLikelihood=266008.2934528439\n",
      "Logistic Regression (99/99): loss logLikelihood=257704.67949372664\n",
      "Logistic Regression (99/99): loss logLikelihood=264375.30283985764\n",
      "Logistic Regression (99/99): loss logLikelihood=265177.0568355593\n",
      "Logistic Regression (99/99): loss logLikelihood=244306.8533761944\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_lep_eta_centrality (index : 12 )\n",
      "Logistic Regression (99/99): loss logLikelihood=258555.7945728625\n",
      "Logistic Regression (99/99): loss logLikelihood=305399.56342955213\n",
      "Logistic Regression (99/99): loss logLikelihood=261895.715197596\n",
      "Logistic Regression (99/99): loss logLikelihood=233813.8576448669\n",
      "Logistic Regression (99/99): loss logLikelihood=260897.7672422656\n",
      "Logistic Regression (99/99): loss logLikelihood=250867.24704450904\n",
      "Logistic Regression (99/99): loss logLikelihood=259631.42646363325\n",
      "Logistic Regression (99/99): loss logLikelihood=267516.0340905529\n",
      "Logistic Regression (99/99): loss logLikelihood=256746.57822795952\n",
      "Logistic Regression (99/99): loss logLikelihood=236147.84564858692\n",
      "Logistic Regression (99/99): loss logLikelihood=283264.5643454686\n",
      "Logistic Regression (99/99): loss logLikelihood=232253.20859105937\n",
      "Logistic Regression (99/99): loss logLikelihood=231201.55640116727\n",
      "Logistic Regression (99/99): loss logLikelihood=266771.19894869917\n",
      "Logistic Regression (99/99): loss logLikelihood=266998.3156379259\n",
      "Logistic Regression (99/99): loss logLikelihood=262171.23327436217\n",
      "Logistic Regression (99/99): loss logLikelihood=266807.88349798985\n",
      "Logistic Regression (99/99): loss logLikelihood=266238.2101656805\n",
      "Logistic Regression (99/99): loss logLikelihood=254675.65531689874\n",
      "Logistic Regression (99/99): loss logLikelihood=265683.2433138422\n",
      "Logistic Regression (99/99): loss logLikelihood=239238.0078359134\n",
      "Logistic Regression (99/99): loss logLikelihood=238849.50584246652\n",
      "Logistic Regression (99/99): loss logLikelihood=254652.6796581743\n",
      "Logistic Regression (99/99): loss logLikelihood=263824.232403926\n",
      "Logistic Regression (99/99): loss logLikelihood=264926.5583527673\n",
      "Logistic Regression (99/99): loss logLikelihood=258581.20822348777\n",
      "Logistic Regression (99/99): loss logLikelihood=259308.52021636\n",
      "Logistic Regression (99/99): loss logLikelihood=261389.60950253237\n",
      "Logistic Regression (99/99): loss logLikelihood=237828.99153429514\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltaeta_jet_jet (index : 4 )\n",
      "Logistic Regression (99/99): loss logLikelihood=273049.6762225675\n",
      "Logistic Regression (99/99): loss logLikelihood=319614.9051492282\n",
      "Logistic Regression (99/99): loss logLikelihood=276512.055015867\n",
      "Logistic Regression (99/99): loss logLikelihood=243007.8574109573\n",
      "Logistic Regression (99/99): loss logLikelihood=273894.12367819995\n",
      "Logistic Regression (99/99): loss logLikelihood=252751.61728955896\n",
      "Logistic Regression (99/99): loss logLikelihood=283533.93744624156\n",
      "Logistic Regression (99/99): loss logLikelihood=267189.2802714217\n",
      "Logistic Regression (99/99): loss logLikelihood=243791.32339164132\n",
      "Logistic Regression (99/99): loss logLikelihood=296042.0756728374\n",
      "Logistic Regression (99/99): loss logLikelihood=244345.2551838523\n",
      "Logistic Regression (99/99): loss logLikelihood=244253.5251539057\n",
      "Logistic Regression (99/99): loss logLikelihood=280844.3157832216\n",
      "Logistic Regression (99/99): loss logLikelihood=280970.94547032873\n",
      "Logistic Regression (99/99): loss logLikelihood=274095.7995841243\n",
      "Logistic Regression (99/99): loss logLikelihood=280746.2133725927\n",
      "Logistic Regression (99/99): loss logLikelihood=280241.4853742214\n",
      "Logistic Regression (99/99): loss logLikelihood=265592.7237053507\n",
      "Logistic Regression (99/99): loss logLikelihood=279594.1385259245\n",
      "Logistic Regression (99/99): loss logLikelihood=247003.58227762108\n",
      "Logistic Regression (99/99): loss logLikelihood=244259.74777486525\n",
      "Logistic Regression (99/99): loss logLikelihood=267164.62302930263\n",
      "Logistic Regression (99/99): loss logLikelihood=276533.99489598745\n",
      "Logistic Regression (99/99): loss logLikelihood=277764.9451954343\n",
      "Logistic Regression (99/99): loss logLikelihood=270876.1241290154\n",
      "Logistic Regression (99/99): loss logLikelihood=268589.12852381595\n",
      "Logistic Regression (99/99): loss logLikelihood=271438.1834163098\n",
      "Logistic Regression (99/99): loss logLikelihood=245183.87188390308\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "Logistic Regression (99/99): loss logLikelihood=285289.09049744136\n",
      "Logistic Regression (99/99): loss logLikelihood=332418.0139865653\n",
      "Logistic Regression (99/99): loss logLikelihood=288822.15323871985\n",
      "Logistic Regression (99/99): loss logLikelihood=253108.44290293043\n",
      "Logistic Regression (99/99): loss logLikelihood=258723.77394538475\n",
      "Logistic Regression (99/99): loss logLikelihood=296689.55787687516\n",
      "Logistic Regression (99/99): loss logLikelihood=278194.85915477667\n",
      "Logistic Regression (99/99): loss logLikelihood=253858.0146239652\n",
      "Logistic Regression (99/99): loss logLikelihood=307757.00374456\n",
      "Logistic Regression (99/99): loss logLikelihood=255286.37752632497\n",
      "Logistic Regression (99/99): loss logLikelihood=256095.13577829153\n",
      "Logistic Regression (99/99): loss logLikelihood=293223.90381066897\n",
      "Logistic Regression (99/99): loss logLikelihood=293419.6176419491\n",
      "Logistic Regression (99/99): loss logLikelihood=285526.19650955405\n",
      "Logistic Regression (99/99): loss logLikelihood=293048.31065607944\n",
      "Logistic Regression (99/99): loss logLikelihood=292671.6937016157\n",
      "Logistic Regression (99/99): loss logLikelihood=276796.9028138518\n",
      "Logistic Regression (99/99): loss logLikelihood=292009.1138665114\n",
      "Logistic Regression (99/99): loss logLikelihood=257117.39922518333\n",
      "Logistic Regression (99/99): loss logLikelihood=251264.3906651737\n",
      "Logistic Regression (99/99): loss logLikelihood=281248.4239967243\n",
      "Logistic Regression (99/99): loss logLikelihood=288862.1639994207\n",
      "Logistic Regression (99/99): loss logLikelihood=289872.25908906676\n",
      "Logistic Regression (99/99): loss logLikelihood=286157.45624108426\n",
      "Logistic Regression (99/99): loss logLikelihood=279960.54753954394\n",
      "Logistic Regression (99/99): loss logLikelihood=282523.49752699904\n",
      "Logistic Regression (99/99): loss logLikelihood=255126.0910482065\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "Logistic Regression (99/99): loss logLikelihood=317514.6060759758\n",
      "Logistic Regression (99/99): loss logLikelihood=351690.5157065311\n",
      "Logistic Regression (99/99): loss logLikelihood=326427.37089864846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (99/99): loss logLikelihood=262506.6088577243\n",
      "Logistic Regression (99/99): loss logLikelihood=280905.4684857379\n",
      "Logistic Regression (99/99): loss logLikelihood=296810.7079013575\n",
      "Logistic Regression (99/99): loss logLikelihood=266558.23132176045\n",
      "Logistic Regression (99/99): loss logLikelihood=330474.4011851847\n",
      "Logistic Regression (99/99): loss logLikelihood=271218.13241436967\n",
      "Logistic Regression (99/99): loss logLikelihood=274918.6528522846\n",
      "Logistic Regression (99/99): loss logLikelihood=313325.3300276488\n",
      "Logistic Regression (99/99): loss logLikelihood=313185.2762982435\n",
      "Logistic Regression (99/99): loss logLikelihood=306941.5605074641\n",
      "Logistic Regression (99/99): loss logLikelihood=312919.48377479205\n",
      "Logistic Regression (99/99): loss logLikelihood=312853.15570574335\n",
      "Logistic Regression (99/99): loss logLikelihood=287689.8681949731\n",
      "Logistic Regression (99/99): loss logLikelihood=312215.3139059107\n",
      "Logistic Regression (99/99): loss logLikelihood=269723.49942221155\n",
      "Logistic Regression (99/99): loss logLikelihood=264777.9921261508\n",
      "Logistic Regression (99/99): loss logLikelihood=297260.09066385526\n",
      "Logistic Regression (99/99): loss logLikelihood=310606.7472068149\n",
      "Logistic Regression (99/99): loss logLikelihood=311920.5989877415\n",
      "Logistic Regression (99/99): loss logLikelihood=306897.48747046554\n",
      "Logistic Regression (99/99): loss logLikelihood=303037.4537957257\n",
      "Logistic Regression (99/99): loss logLikelihood=305247.5526962065\n",
      "Logistic Regression (99/99): loss logLikelihood=266860.25900423067\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_phi (index : 25 )\n",
      "Logistic Regression (99/99): loss logLikelihood=332011.77483705257\n",
      "Logistic Regression (99/99): loss logLikelihood=366818.4607416779\n",
      "Logistic Regression (99/99): loss logLikelihood=340813.73092534585\n",
      "Logistic Regression (99/99): loss logLikelihood=277625.75132982054\n",
      "Logistic Regression (99/99): loss logLikelihood=297445.07272242446\n",
      "Logistic Regression (99/99): loss logLikelihood=312217.098893572\n",
      "Logistic Regression (99/99): loss logLikelihood=282333.1394231377\n",
      "Logistic Regression (99/99): loss logLikelihood=345169.8687896129\n",
      "Logistic Regression (99/99): loss logLikelihood=284939.7088784134\n",
      "Logistic Regression (99/99): loss logLikelihood=289998.5026690504\n",
      "Logistic Regression (99/99): loss logLikelihood=327708.5190531592\n",
      "Logistic Regression (99/99): loss logLikelihood=323421.8667388055\n",
      "Logistic Regression (99/99): loss logLikelihood=322018.8357635362\n",
      "Logistic Regression (99/99): loss logLikelihood=327284.29251014424\n",
      "Logistic Regression (99/99): loss logLikelihood=320975.0046605313\n",
      "Logistic Regression (99/99): loss logLikelihood=303201.42316098476\n",
      "Logistic Regression (99/99): loss logLikelihood=320574.29016870476\n",
      "Logistic Regression (99/99): loss logLikelihood=285385.10247387754\n",
      "Logistic Regression (99/99): loss logLikelihood=280270.79066846904\n",
      "Logistic Regression (99/99): loss logLikelihood=313886.7667560618\n",
      "Logistic Regression (99/99): loss logLikelihood=324462.39979664836\n",
      "Logistic Regression (99/99): loss logLikelihood=322032.9574623022\n",
      "Logistic Regression (99/99): loss logLikelihood=318343.6976306582\n",
      "Logistic Regression (99/99): loss logLikelihood=319460.52420014446\n",
      "Logistic Regression (99/99): loss logLikelihood=282717.7847920389\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "Logistic Regression (99/99): loss logLikelihood=370764.0097241576\n",
      "Logistic Regression (99/99): loss logLikelihood=396052.6307810475\n",
      "Logistic Regression (99/99): loss logLikelihood=307235.1155140362\n",
      "Logistic Regression (99/99): loss logLikelihood=327327.16319344664\n",
      "Logistic Regression (99/99): loss logLikelihood=341682.8269497436\n",
      "Logistic Regression (99/99): loss logLikelihood=315638.88696333696\n",
      "Logistic Regression (99/99): loss logLikelihood=374584.7844160171\n",
      "Logistic Regression (99/99): loss logLikelihood=313433.81353287527\n",
      "Logistic Regression (99/99): loss logLikelihood=324881.18310398405\n",
      "Logistic Regression (99/99): loss logLikelihood=355515.89641822723\n",
      "Logistic Regression (99/99): loss logLikelihood=351422.59242848336\n",
      "Logistic Regression (99/99): loss logLikelihood=358264.90289692814\n",
      "Logistic Regression (99/99): loss logLikelihood=354992.23362255516\n",
      "Logistic Regression (99/99): loss logLikelihood=349218.72517693613\n",
      "Logistic Regression (99/99): loss logLikelihood=330453.576763183\n",
      "Logistic Regression (99/99): loss logLikelihood=348579.4441294865\n",
      "Logistic Regression (99/99): loss logLikelihood=317608.5256343109\n",
      "Logistic Regression (99/99): loss logLikelihood=311819.5408971453\n",
      "Logistic Regression (99/99): loss logLikelihood=341097.6332302743\n",
      "Logistic Regression (99/99): loss logLikelihood=352764.93702371523\n",
      "Logistic Regression (99/99): loss logLikelihood=350048.74318429915\n",
      "Logistic Regression (99/99): loss logLikelihood=347231.23944369226\n",
      "Logistic Regression (99/99): loss logLikelihood=348208.80908701854\n",
      "Logistic Regression (99/99): loss logLikelihood=313083.99562698416\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_eta (index : 24 )\n",
      "Logistic Regression (99/99): loss logLikelihood=382095.1517204504\n",
      "Logistic Regression (99/99): loss logLikelihood=407986.8823160142\n",
      "Logistic Regression (99/99): loss logLikelihood=319311.2531386779\n",
      "Logistic Regression (99/99): loss logLikelihood=340683.24388123583\n",
      "Logistic Regression (99/99): loss logLikelihood=353453.81342541095\n",
      "Logistic Regression (99/99): loss logLikelihood=327740.934342332\n",
      "Logistic Regression (99/99): loss logLikelihood=386462.8989281917\n",
      "Logistic Regression (99/99): loss logLikelihood=324538.4926069413\n",
      "Logistic Regression (99/99): loss logLikelihood=336491.9978340739\n",
      "Logistic Regression (99/99): loss logLikelihood=371638.33122511784\n",
      "Logistic Regression (99/99): loss logLikelihood=363169.3550483399\n",
      "Logistic Regression (99/99): loss logLikelihood=370070.696540925\n",
      "Logistic Regression (99/99): loss logLikelihood=371267.945630148\n",
      "Logistic Regression (99/99): loss logLikelihood=361085.4569707917\n",
      "Logistic Regression (99/99): loss logLikelihood=342585.0967505928\n",
      "Logistic Regression (99/99): loss logLikelihood=360380.2837532795\n",
      "Logistic Regression (99/99): loss logLikelihood=329689.81555716816\n",
      "Logistic Regression (99/99): loss logLikelihood=323667.240772478\n",
      "Logistic Regression (99/99): loss logLikelihood=353751.2032932555\n",
      "Logistic Regression (99/99): loss logLikelihood=361977.11473996914\n",
      "Logistic Regression (99/99): loss logLikelihood=359195.15628048056\n",
      "Logistic Regression (99/99): loss logLikelihood=360301.0927929748\n",
      "Logistic Regression (99/99): loss logLikelihood=325274.4018176785\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_pt (index : 23 )\n",
      "Logistic Regression (99/99): loss logLikelihood=382585.1059775346\n",
      "Logistic Regression (99/99): loss logLikelihood=408081.0311255709\n",
      "Logistic Regression (99/99): loss logLikelihood=328371.89706322737\n",
      "Logistic Regression (99/99): loss logLikelihood=341960.1809639436\n",
      "Logistic Regression (99/99): loss logLikelihood=356806.533261256\n",
      "Logistic Regression (99/99): loss logLikelihood=337236.42697511095\n",
      "Logistic Regression (99/99): loss logLikelihood=387911.98070601467\n",
      "Logistic Regression (99/99): loss logLikelihood=324848.4157798375\n",
      "Logistic Regression (99/99): loss logLikelihood=341431.7422160744\n",
      "Logistic Regression (99/99): loss logLikelihood=372124.9124610309\n",
      "Logistic Regression (99/99): loss logLikelihood=363902.17944960616\n",
      "Logistic Regression (99/99): loss logLikelihood=374521.5797966339\n",
      "Logistic Regression (99/99): loss logLikelihood=371684.3359527943\n",
      "Logistic Regression (99/99): loss logLikelihood=361547.5265164035\n",
      "Logistic Regression (99/99): loss logLikelihood=351629.3700928909\n",
      "Logistic Regression (99/99): loss logLikelihood=360802.58519671106\n",
      "Logistic Regression (99/99): loss logLikelihood=338297.00283415476\n",
      "Logistic Regression (99/99): loss logLikelihood=322397.6863504209\n",
      "Logistic Regression (99/99): loss logLikelihood=368501.47558067815\n",
      "Logistic Regression (99/99): loss logLikelihood=359833.1257352562\n",
      "Logistic Regression (99/99): loss logLikelihood=361224.0702529883\n",
      "Logistic Regression (99/99): loss logLikelihood=334483.5864598014\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_pt (index : 26 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (99/99): loss logLikelihood=396589.0457221904\n",
      "Logistic Regression (99/99): loss logLikelihood=422340.06002543296\n",
      "Logistic Regression (99/99): loss logLikelihood=345447.67460412474\n",
      "Logistic Regression (99/99): loss logLikelihood=357426.3257070499\n",
      "Logistic Regression (99/99): loss logLikelihood=373851.64021158847\n",
      "Logistic Regression (99/99): loss logLikelihood=355543.03457364975\n",
      "Logistic Regression (99/99): loss logLikelihood=402589.33478416956\n",
      "Logistic Regression (99/99): loss logLikelihood=339832.82709944167\n",
      "Logistic Regression (99/99): loss logLikelihood=357046.7212753518\n",
      "Logistic Regression (99/99): loss logLikelihood=386581.8019907323\n",
      "Logistic Regression (99/99): loss logLikelihood=378552.29695135035\n",
      "Logistic Regression (99/99): loss logLikelihood=389897.6109170226\n",
      "Logistic Regression (99/99): loss logLikelihood=386174.32341035217\n",
      "Logistic Regression (99/99): loss logLikelihood=376209.99546717486\n",
      "Logistic Regression (99/99): loss logLikelihood=368430.2636189972\n",
      "Logistic Regression (99/99): loss logLikelihood=375504.70068128436\n",
      "Logistic Regression (99/99): loss logLikelihood=356285.79027530603\n",
      "Logistic Regression (99/99): loss logLikelihood=337363.8751479099\n",
      "Logistic Regression (99/99): loss logLikelihood=373880.2517208347\n",
      "Logistic Regression (99/99): loss logLikelihood=375478.25979497004\n",
      "Logistic Regression (99/99): loss logLikelihood=352953.1457938132\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_eta (index : 14 )\n",
      "Logistic Regression (99/99): loss logLikelihood=414180.5373174051\n",
      "Logistic Regression (99/99): loss logLikelihood=439793.66708282946\n",
      "Logistic Regression (99/99): loss logLikelihood=364833.9563682707\n",
      "Logistic Regression (99/99): loss logLikelihood=376463.70983500825\n",
      "Logistic Regression (99/99): loss logLikelihood=392248.6451402833\n",
      "Logistic Regression (99/99): loss logLikelihood=374772.76981586457\n",
      "Logistic Regression (99/99): loss logLikelihood=420275.6268364077\n",
      "Logistic Regression (99/99): loss logLikelihood=358592.18965656427\n",
      "Logistic Regression (99/99): loss logLikelihood=375061.2739333559\n",
      "Logistic Regression (99/99): loss logLikelihood=396209.0155488249\n",
      "Logistic Regression (99/99): loss logLikelihood=407639.8635141861\n",
      "Logistic Regression (99/99): loss logLikelihood=420193.13414535933\n",
      "Logistic Regression (99/99): loss logLikelihood=394064.25246177113\n",
      "Logistic Regression (99/99): loss logLikelihood=387321.955523228\n",
      "Logistic Regression (99/99): loss logLikelihood=393422.5140026219\n",
      "Logistic Regression (99/99): loss logLikelihood=375420.6459803514\n",
      "Logistic Regression (99/99): loss logLikelihood=357727.7683284654\n",
      "Logistic Regression (99/99): loss logLikelihood=393181.5513903972\n",
      "Logistic Regression (99/99): loss logLikelihood=393502.388338244\n",
      "Logistic Regression (99/99): loss logLikelihood=372512.0808836641\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_eta (index : 17 )\n",
      "Logistic Regression (99/99): loss logLikelihood=447041.6680034232\n",
      "Logistic Regression (99/99): loss logLikelihood=472462.34110807785\n",
      "Logistic Regression (99/99): loss logLikelihood=400332.03748509707\n",
      "Logistic Regression (99/99): loss logLikelihood=411389.6919418632\n",
      "Logistic Regression (99/99): loss logLikelihood=426212.7997939567\n",
      "Logistic Regression (99/99): loss logLikelihood=409931.1106218888\n",
      "Logistic Regression (99/99): loss logLikelihood=453339.1940746121\n",
      "Logistic Regression (99/99): loss logLikelihood=392778.51801866916\n",
      "Logistic Regression (99/99): loss logLikelihood=408029.7468874393\n",
      "Logistic Regression (99/99): loss logLikelihood=428688.49349279003\n",
      "Logistic Regression (99/99): loss logLikelihood=440520.5436774419\n",
      "Logistic Regression (99/99): loss logLikelihood=426917.86798998626\n",
      "Logistic Regression (99/99): loss logLikelihood=422227.6349859243\n",
      "Logistic Regression (99/99): loss logLikelihood=426318.4113811679\n",
      "Logistic Regression (99/99): loss logLikelihood=410502.89348937036\n",
      "Logistic Regression (99/99): loss logLikelihood=394666.2327809512\n",
      "Logistic Regression (99/99): loss logLikelihood=427971.4748907567\n",
      "Logistic Regression (99/99): loss logLikelihood=426984.68922086846\n",
      "Logistic Regression (99/99): loss logLikelihood=408269.8717047935\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_MMC (index : 0 )\n",
      "Logistic Regression (99/99): loss logLikelihood=501085.82804612356\n",
      "Logistic Regression (99/99): loss logLikelihood=428226.07829664805\n",
      "Logistic Regression (99/99): loss logLikelihood=438919.6401377064\n",
      "Logistic Regression (99/99): loss logLikelihood=453168.5382642357\n",
      "Logistic Regression (99/99): loss logLikelihood=439183.2472171386\n",
      "Logistic Regression (99/99): loss logLikelihood=480181.01070794906\n",
      "Logistic Regression (99/99): loss logLikelihood=420478.0488802069\n",
      "Logistic Regression (99/99): loss logLikelihood=438029.39323351794\n",
      "Logistic Regression (99/99): loss logLikelihood=455290.259538763\n",
      "Logistic Regression (99/99): loss logLikelihood=470682.4429307158\n",
      "Logistic Regression (99/99): loss logLikelihood=453555.90082629805\n",
      "Logistic Regression (99/99): loss logLikelihood=451211.502168422\n",
      "Logistic Regression (99/99): loss logLikelihood=452913.72957522486\n",
      "Logistic Regression (99/99): loss logLikelihood=439098.9874140875\n",
      "Logistic Regression (99/99): loss logLikelihood=423393.8699401015\n",
      "Logistic Regression (99/99): loss logLikelihood=454478.62091419747\n",
      "Logistic Regression (99/99): loss logLikelihood=453634.68276132195\n",
      "Logistic Regression (99/99): loss logLikelihood=436267.63896413153\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "Logistic Regression (99/99): loss logLikelihood=488687.8915398743\n",
      "Logistic Regression (99/99): loss logLikelihood=424735.7618963204\n",
      "Logistic Regression (99/99): loss logLikelihood=430783.429205196\n",
      "Logistic Regression (99/99): loss logLikelihood=445771.23478401947\n",
      "Logistic Regression (99/99): loss logLikelihood=436747.680298282\n",
      "Logistic Regression (99/99): loss logLikelihood=463047.50859865244\n",
      "Logistic Regression (99/99): loss logLikelihood=414286.2331238239\n",
      "Logistic Regression (99/99): loss logLikelihood=446325.143000658\n",
      "Logistic Regression (99/99): loss logLikelihood=462146.56781269045\n",
      "Logistic Regression (99/99): loss logLikelihood=444454.46856106527\n",
      "Logistic Regression (99/99): loss logLikelihood=444245.9559954833\n",
      "Logistic Regression (99/99): loss logLikelihood=443855.4701359538\n",
      "Logistic Regression (99/99): loss logLikelihood=436330.50741112913\n",
      "Logistic Regression (99/99): loss logLikelihood=418917.8016182697\n",
      "Logistic Regression (99/99): loss logLikelihood=445337.17071231\n",
      "Logistic Regression (99/99): loss logLikelihood=444439.02834696154\n",
      "Logistic Regression (99/99): loss logLikelihood=432004.0139544791\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n",
      "Logistic Regression (99/99): loss logLikelihood=494966.1610832099\n",
      "Logistic Regression (99/99): loss logLikelihood=431399.2988424773\n",
      "Logistic Regression (99/99): loss logLikelihood=437235.79335376306\n",
      "Logistic Regression (99/99): loss logLikelihood=452106.6734926725\n",
      "Logistic Regression (99/99): loss logLikelihood=443318.90919923905\n",
      "Logistic Regression (99/99): loss logLikelihood=469552.74227343686\n",
      "Logistic Regression (99/99): loss logLikelihood=420778.33340395935\n",
      "Logistic Regression (99/99): loss logLikelihood=447530.6038966421\n",
      "Logistic Regression (99/99): loss logLikelihood=468516.9537981303\n",
      "Logistic Regression (99/99): loss logLikelihood=450833.8578376176\n",
      "Logistic Regression (99/99): loss logLikelihood=449487.73590116645\n",
      "Logistic Regression (99/99): loss logLikelihood=442785.92501182994\n",
      "Logistic Regression (99/99): loss logLikelihood=425675.5902507399\n",
      "Logistic Regression (99/99): loss logLikelihood=451689.64203996194\n",
      "Logistic Regression (99/99): loss logLikelihood=449839.11698853667\n",
      "Logistic Regression (99/99): loss logLikelihood=438679.16404136\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_pt (index : 16 )\n",
      "Logistic Regression (99/99): loss logLikelihood=522962.5359100101\n",
      "Logistic Regression (99/99): loss logLikelihood=460158.0052037037\n",
      "Logistic Regression (99/99): loss logLikelihood=462457.7047873484\n",
      "Logistic Regression (99/99): loss logLikelihood=477689.97120056726\n",
      "Logistic Regression (99/99): loss logLikelihood=473309.86439751927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (99/99): loss logLikelihood=503512.2356915283\n",
      "Logistic Regression (99/99): loss logLikelihood=445853.8363746724\n",
      "Logistic Regression (99/99): loss logLikelihood=471345.770908021\n",
      "Logistic Regression (99/99): loss logLikelihood=477014.1386390611\n",
      "Logistic Regression (99/99): loss logLikelihood=473672.35845516296\n",
      "Logistic Regression (99/99): loss logLikelihood=471580.6030958575\n",
      "Logistic Regression (99/99): loss logLikelihood=453902.0133380223\n",
      "Logistic Regression (99/99): loss logLikelihood=475594.9193112883\n",
      "Logistic Regression (99/99): loss logLikelihood=473763.843801573\n",
      "Logistic Regression (99/99): loss logLikelihood=466888.4879183331\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "Logistic Regression (99/99): loss logLikelihood=510781.65449846804\n",
      "Logistic Regression (99/99): loss logLikelihood=517592.4880987175\n",
      "Logistic Regression (99/99): loss logLikelihood=531585.1507575389\n",
      "Logistic Regression (99/99): loss logLikelihood=525236.268945912\n",
      "Logistic Regression (99/99): loss logLikelihood=563333.0045232446\n",
      "Logistic Regression (99/99): loss logLikelihood=492017.7457579444\n",
      "Logistic Regression (99/99): loss logLikelihood=525866.7370387822\n",
      "Logistic Regression (99/99): loss logLikelihood=535227.1529240304\n",
      "Logistic Regression (99/99): loss logLikelihood=528009.0702258466\n",
      "Logistic Regression (99/99): loss logLikelihood=523199.1491598388\n",
      "Logistic Regression (99/99): loss logLikelihood=505090.5078262257\n",
      "Logistic Regression (99/99): loss logLikelihood=529829.5997560332\n",
      "Logistic Regression (99/99): loss logLikelihood=528215.0503727761\n",
      "Logistic Regression (99/99): loss logLikelihood=518255.5121005348\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_ratio_lep_tau (index : 10 )\n",
      "Logistic Regression (99/99): loss logLikelihood=552091.5628622234\n",
      "Logistic Regression (99/99): loss logLikelihood=558859.1113576661\n",
      "Logistic Regression (99/99): loss logLikelihood=572281.1037804066\n",
      "Logistic Regression (99/99): loss logLikelihood=566600.4790244119\n",
      "Logistic Regression (99/99): loss logLikelihood=531753.3950277966\n",
      "Logistic Regression (99/99): loss logLikelihood=566112.0087570483\n",
      "Logistic Regression (99/99): loss logLikelihood=576220.9016755527\n",
      "Logistic Regression (99/99): loss logLikelihood=568346.5015683577\n",
      "Logistic Regression (99/99): loss logLikelihood=563891.806658438\n",
      "Logistic Regression (99/99): loss logLikelihood=546856.8651303292\n",
      "Logistic Regression (99/99): loss logLikelihood=569977.540164825\n",
      "Logistic Regression (99/99): loss logLikelihood=568486.1098449808\n",
      "Logistic Regression (99/99): loss logLikelihood=559640.7000717035\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_phi (index : 20 )\n",
      "Logistic Regression (99/99): loss logLikelihood=557095.0486637502\n",
      "Logistic Regression (99/99): loss logLikelihood=564195.33570403\n",
      "Logistic Regression (99/99): loss logLikelihood=577253.0680446385\n",
      "Logistic Regression (99/99): loss logLikelihood=571487.176167892\n",
      "Logistic Regression (99/99): loss logLikelihood=536916.839050614\n",
      "Logistic Regression (99/99): loss logLikelihood=570763.3468010363\n",
      "Logistic Regression (99/99): loss logLikelihood=581062.0158950731\n",
      "Logistic Regression (99/99): loss logLikelihood=568795.7056424015\n",
      "Logistic Regression (99/99): loss logLikelihood=551888.0037511761\n",
      "Logistic Regression (99/99): loss logLikelihood=574929.4281902817\n",
      "Logistic Regression (99/99): loss logLikelihood=572635.0539099401\n",
      "Logistic Regression (99/99): loss logLikelihood=564611.1081531795\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_sum_pt (index : 9 )\n",
      "Logistic Regression (99/99): loss logLikelihood=570279.4263917711\n",
      "Logistic Regression (99/99): loss logLikelihood=568916.5399843153\n",
      "Logistic Regression (99/99): loss logLikelihood=584758.6929442458\n",
      "Logistic Regression (99/99): loss logLikelihood=547212.1726811457\n",
      "Logistic Regression (99/99): loss logLikelihood=573948.8223051378\n",
      "Logistic Regression (99/99): loss logLikelihood=589989.2770068669\n",
      "Logistic Regression (99/99): loss logLikelihood=582380.0179520481\n",
      "Logistic Regression (99/99): loss logLikelihood=567144.3819166127\n",
      "Logistic Regression (99/99): loss logLikelihood=578048.0222067883\n",
      "Logistic Regression (99/99): loss logLikelihood=575576.1690114415\n",
      "Logistic Regression (99/99): loss logLikelihood=578952.7003538295\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_sumet (index : 21 )\n",
      "Logistic Regression (99/99): loss logLikelihood=589849.1063197406\n",
      "Logistic Regression (99/99): loss logLikelihood=581424.9994277575\n",
      "Logistic Regression (99/99): loss logLikelihood=601217.7069571012\n",
      "Logistic Regression (99/99): loss logLikelihood=564945.5963315491\n",
      "Logistic Regression (99/99): loss logLikelihood=584930.1338786059\n",
      "Logistic Regression (99/99): loss logLikelihood=605698.7470827653\n",
      "Logistic Regression (99/99): loss logLikelihood=588434.0659379255\n",
      "Logistic Regression (99/99): loss logLikelihood=588897.7193945546\n",
      "Logistic Regression (99/99): loss logLikelihood=586280.0972544486\n",
      "Logistic Regression (99/99): loss logLikelihood=598923.6223236439\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_all_pt (index : 29 )\n",
      "Logistic Regression (99/99): loss logLikelihood=614257.0849608945\n",
      "Logistic Regression (99/99): loss logLikelihood=599464.8483458253\n",
      "Logistic Regression (99/99): loss logLikelihood=621020.7361549685\n",
      "Logistic Regression (99/99): loss logLikelihood=587868.0780782908\n",
      "Logistic Regression (99/99): loss logLikelihood=601496.4611690356\n",
      "Logistic Regression (99/99): loss logLikelihood=626789.9086149529\n",
      "Logistic Regression (99/99): loss logLikelihood=615591.3742806973\n",
      "Logistic Regression (99/99): loss logLikelihood=605334.3917901807\n",
      "Logistic Regression (99/99): loss logLikelihood=602616.9934340012\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_num (index : 22 )\n",
      "Logistic Regression (99/99): loss logLikelihood=638675.96154897\n",
      "Logistic Regression (99/99): loss logLikelihood=618216.0338098371\n",
      "Logistic Regression (99/99): loss logLikelihood=641357.9525419341\n",
      "Logistic Regression (99/99): loss logLikelihood=611889.3811605065\n",
      "Logistic Regression (99/99): loss logLikelihood=618214.8930368742\n",
      "Logistic Regression (99/99): loss logLikelihood=647087.9158386507\n",
      "Logistic Regression (99/99): loss logLikelihood=622176.2191643673\n",
      "Logistic Regression (99/99): loss logLikelihood=619163.9204936146\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_h (index : 3 )\n",
      "Logistic Regression (99/99): loss logLikelihood=642326.3463453004\n",
      "Logistic Regression (99/99): loss logLikelihood=666955.0658335374\n",
      "Logistic Regression (99/99): loss logLikelihood=641163.0367050774\n",
      "Logistic Regression (99/99): loss logLikelihood=641261.2470471336\n",
      "Logistic Regression (99/99): loss logLikelihood=674166.6270939113\n",
      "Logistic Regression (99/99): loss logLikelihood=645043.0792178708\n",
      "Logistic Regression (99/99): loss logLikelihood=642013.635351324\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "Logistic Regression (99/99): loss logLikelihood=645411.1292474177\n",
      "Logistic Regression (99/99): loss logLikelihood=671322.0981463399\n",
      "Logistic Regression (99/99): loss logLikelihood=643649.9113394426\n",
      "Logistic Regression (99/99): loss logLikelihood=676957.3276050666\n",
      "Logistic Regression (99/99): loss logLikelihood=647426.6550218659\n",
      "Logistic Regression (99/99): loss logLikelihood=644325.7553609771\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "Logistic Regression (99/99): loss logLikelihood=681812.4866216923\n",
      "Logistic Regression (99/99): loss logLikelihood=708115.2955984657\n",
      "Logistic Regression (99/99): loss logLikelihood=679442.2391773809\n",
      "Logistic Regression (99/99): loss logLikelihood=682996.2866427448\n",
      "Logistic Regression (99/99): loss logLikelihood=680005.3184550253\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "Logistic Regression (99/99): loss logLikelihood=713868.6339168998\n",
      "Logistic Regression (99/99): loss logLikelihood=710509.2740011092\n",
      "Logistic Regression (99/99): loss logLikelihood=713959.2998046542\n",
      "Logistic Regression (99/99): loss logLikelihood=711109.0099301715\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_eta (index : 27 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (99/99): loss logLikelihood=719419.2357479985\n",
      "Logistic Regression (99/99): loss logLikelihood=716345.7384532014\n",
      "Logistic Regression (99/99): loss logLikelihood=716843.6181024022\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_phi (index : 15 )\n",
      "Logistic Regression (99/99): loss logLikelihood=721835.7496206868\n",
      "Logistic Regression (99/99): loss logLikelihood=718793.5101705276\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n",
      "Logistic Regression (99/99): loss logLikelihood=724468.4282056518\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "initial_w = np.zeros(X.shape[1])\n",
    "\n",
    "w0, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "y = predict_labels(w0[-1], X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0[-1]))) - y*(X.dot(w0[-1])))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        initial_w = np.ones(X.shape[1])\n",
    "        ws, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "        y = predict_labels(ws[-1], X)\n",
    "        loglike = compute_loglikelihood_reg(y, X, ws[-1])\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.584989564844\n",
      "0.628490335758\n",
      "0.637035324763\n",
      "0.645088487884\n",
      "0.668335217889\n",
      "0.688641072632\n",
      "0.702348484082\n",
      "0.716077073489\n",
      "0.719137199731\n",
      "0.721952626232\n",
      "0.751999127694\n",
      "0.762099550756\n",
      "0.767221298259\n",
      "0.772003396898\n",
      "0.777005030507\n",
      "0.780513526289\n",
      "0.786122320624\n",
      "0.790202346179\n",
      "0.794030817651\n",
      "0.801993335539\n",
      "0.810886692513\n",
      "0.823346800042\n",
      "0.837219034915\n",
      "0.848164971197\n",
      "0.857199628813\n",
      "0.86269623648\n",
      "0.863923871446\n",
      "0.86472468272\n",
      "0.865288024327\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 29\n",
      "\n",
      "\n",
      "Indices of features chosen:  [12, 4, 5, 7, 25, 2, 24, 23, 26, 14, 17, 0, 13, 18, 16, 1, 10, 20, 9, 21, 29, 22, 3, 11, 19, 8, 27, 15, 28]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With cross correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_lr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 0\n",
      "\n",
      "\n",
      "Indices of features chosen:  []\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DER_prodeta_jet_jet'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
