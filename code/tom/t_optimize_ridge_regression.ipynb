{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plots import *\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_lambda_ridge_regression(y, x, degree, lambda_min, lambda_max, lambda_steps, k_fold, seed_split_data):\n",
    "    \"\"\" Given a degree for the regression it finds the optimal lambda in the log interval [lambda_min, lambda_max]\n",
    "    thanks to cross   validation on 'k_folds' different training/testing sets. \"\"\"\n",
    "        \n",
    "    # tested lambdas\n",
    "    lambdas = np.logspace(lambda_min, lambda_max, lambda_steps)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed_split_data)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    " \n",
    "    for  lambda_ in lambdas:\n",
    "\n",
    "        rmse_tr_all = []\n",
    "        rmse_te_all = []\n",
    "            \n",
    "        for k in range(k_fold):        \n",
    "        \n",
    "            # compute losses for the k'th fold\n",
    "            rmse_tr_tmp, rmse_te_tmp = cross_validation_ridge_regression(y, x, k_indices, k, lambda_, degree)\n",
    "\n",
    "            # store losses \n",
    "            rmse_tr_all.append(rmse_tr_tmp)\n",
    "            rmse_te_all.append(rmse_te_tmp)\n",
    "        \n",
    "        # store mean losses\n",
    "        rmse_tr.append(np.mean(rmse_tr_all))\n",
    "        rmse_te.append(np.mean(rmse_te_all))\n",
    "    \n",
    "    # extract the optimal value for lambda\n",
    "    lambda_opt = lambdas[rmse_te.index(min(rmse_te))]\n",
    "    \n",
    "    # plot the training and testing errors for the different values of lambda\n",
    "    cross_validation_visualization_lambda(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "    return lambda_opt, rmse_tr, rmse_te\n",
    "\n",
    "def cross_validation_degree_ridge_regression(y, x, lambda_, degree_min, degree_max, k_fold):\n",
    "    \"\"\" Given a degree for the regression it finds the optimal degree in the log interval [degree_min, degree_max]\n",
    "    thanks to cross   validation on 'k_folds' different training/testing sets. \"\"\"\n",
    "        \n",
    "    # tested degrees\n",
    "    degrees = np.arange(degree_min, degree_max+1)\n",
    "    \n",
    "    # split data in k fold\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    " \n",
    "    for  degree in degrees:\n",
    "\n",
    "        rmse_tr_all = []\n",
    "        rmse_te_all = []\n",
    "            \n",
    "        for k in range(k_fold):        \n",
    "        \n",
    "            # compute losses for the k'th fold\n",
    "            rmse_tr_tmp, rmse_te_tmp = cross_validation_ridge_regression(y, x, k_indices, k, lambda_, degree)\n",
    "\n",
    "            # store losses \n",
    "            rmse_tr_all.append(rmse_tr_tmp)\n",
    "            rmse_te_all.append(rmse_te_tmp)\n",
    "        \n",
    "        # store mean losses\n",
    "        rmse_tr.append(np.mean(rmse_tr_all))\n",
    "        rmse_te.append(np.mean(rmse_te_all))\n",
    "    \n",
    "    # extract the optimal degree between 'degree_min' and 'degree_max'\n",
    "    degree_opt = degrees[rmse_te.index(min(rmse_te))]\n",
    "    \n",
    "    # plot the training and testing errors for the different degrees\n",
    "    cross_validation_visualization_degree(degrees, rmse_tr, rmse_te)\n",
    "    \n",
    "    return degree_opt, rmse_tr, rmse_te\n",
    "\n",
    "def cross_validation_rmse_ridge_regression(y, x, lambda_, degree, k_fold):\n",
    "    \"\"\" Giving a linear model it estimates the training and testing errors (rmse) with cross validation. \n",
    "    \n",
    "    The model used is a linear model with a polynomial basis of degree 'degree' gave through ridge regression \n",
    "    whose penalized parameter is 'lambda_'.\n",
    "    \"\"\"\n",
    "   \n",
    "    # split data in k fold\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr_all = []\n",
    "    rmse_te_all = []\n",
    "         \n",
    "    for k in range(k_fold):        \n",
    "        \n",
    "        # compute losses for the k'th fold\n",
    "        rmse_tr_tmp, rmse_te_tmp = cross_validation_ridge_regression(y, x, k_indices, k, lambda_, degree)\n",
    "\n",
    "        # store losses \n",
    "        rmse_tr_all.append(rmse_tr_tmp)\n",
    "        rmse_te_all.append(rmse_te_tmp)\n",
    "        \n",
    "    # std of the losses\n",
    "    mean_rmse_tr = np.mean(rmse_tr_all)\n",
    "    mean_rmse_te = np.mean(rmse_te_all)\n",
    "    \n",
    "    # std of the losses\n",
    "    std_rmse_tr = np.std(rmse_tr_all)\n",
    "    std_rmse_te = np.std(rmse_te_all)\n",
    "    \n",
    "    # boxplot of the training and testing rmse\n",
    "    plt.figure()\n",
    "    plt.boxplot(np.column_stack((np.array(rmse_tr_all), np.array(rmse_te_all))), labels=['training rmse','testing rmse'])\n",
    "    \n",
    "    return mean_rmse_tr, mean_rmse_te, std_rmse_tr, std_rmse_te\n",
    "\n",
    "def cross_validation_classification_ridge_regression(y, x, lambda_, degree, k_fold):\n",
    "    \"\"\" Giving a linear model it estimates the classification error and the class error (rmse) with cross validation. \n",
    "    \n",
    "    The model used is a linear model with a polynomial basis of degree 'degree' gave through ridge regression \n",
    "    whose penalized parameter is 'lambda_'.\n",
    "    \"\"\"\n",
    "        \n",
    "    # split data in k fold\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # class errors and classification errors obtained after prediction on the testing set\n",
    "    classification_errors = []\n",
    "    class_errors = []\n",
    "\n",
    "    for k in range(k_fold):        \n",
    "        \n",
    "        # get k'th subgroup in test, others in train\n",
    "        x_te = x[k_indices[k,:],:]\n",
    "        y_te = y[k_indices[k,:]]\n",
    "        x_tr = x[np.union1d(k_indices[:k,:], k_indices[k+1:,:]),:]\n",
    "        y_tr = y[np.union1d(k_indices[:k,:], k_indices[k+1:,:])]\n",
    "\n",
    "        # build data with polynomial degree\n",
    "        phi_te = build_poly(x_te, degree)\n",
    "        phi_tr = build_poly(x_tr, degree)\n",
    "\n",
    "        # ridge regression\n",
    "        w_tr, loss_tr = ridge_regression(y_tr, phi_tr, lambda_)\n",
    "        \n",
    "        # predict\n",
    "        y_pred = predict_labels(w_tr, phi_te)\n",
    "\n",
    "        # classification error\n",
    "        classification_error = len(np.argwhere(y_te-y_pred))/len(y_te)*100\n",
    "\n",
    "        # class error \n",
    "        class_error_background = len(np.where(np.logical_and(y_te==-1, y_pred==-1))[0])\n",
    "        class_error_signal = len(np.where(np.logical_and(y_te==1, y_pred==1))[0])\n",
    "        class_error = class_error_background/len(np.where(y_te==-1)[0]) + class_error_signal/len(np.where(y_te==1)[0])\n",
    "        \n",
    "        # store\n",
    "        classification_errors.append(classification_error)\n",
    "        class_errors.append(class_error)\n",
    "\n",
    "    # mean and std\n",
    "    mean_classification_error = np.mean(classification_errors)\n",
    "    std_classification_error = np.std(classification_errors)\n",
    "    mean_class_error = np.mean(class_errors)\n",
    "    std_class_error = np.std(class_errors)\n",
    "    \n",
    "    return mean_classification_error, std_classification_error, mean_class_error, std_class_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
