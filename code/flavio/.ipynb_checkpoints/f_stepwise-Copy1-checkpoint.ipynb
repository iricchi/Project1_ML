{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features selection with STEPWISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "my_path = r'C:\\Users\\utente\\Documents\\GitHub\\Project1_ML'\n",
    "sys.path.insert(0,my_path + r'\\code\\COMMON')\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import load_csv_data, predict_labels \n",
    "from implementations import *\n",
    "from outliers import handle_outliers\n",
    "from labels import idx_2labels\n",
    "from standard import standardize\n",
    "from costs import compute_loglikelihood_reg\n",
    "from optimize_hyperparams import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and outliers management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(my_path + r'/data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-999 are replaced by the mean value of the feature\n"
     ]
    }
   ],
   "source": [
    "input_data, Y = handle_outliers(input_data,yb,-999,'mean') # substiution with mean because the standardization\n",
    "                                                           #can be affected, otherwise we should delete the whole row\n",
    "ind_back, ind_sig = idx_2labels(Y, [-1,1])\n",
    "Y[ind_back] = 0\n",
    "\n",
    "input_data, mean_X, std_X = standardize(input_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subdived the X features space in single features\n",
    "all_features = np.genfromtxt(my_path + r'/data/train.csv', delimiter=\",\", dtype=str, max_rows = 1)[2:]\n",
    "# converting array in list in order to simplify the adding of features\n",
    "all_features = list(all_features)\n",
    "\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R^2  as stopping criteria : 1) R2 with error , 2) R2 of Tjur or 3) R2 of McFadden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only way to use the stepwise is using R2 of Tjur or McFadden because of the binary values of the indipendent variable, but the error was also used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def results_r2_stepwise(list_r2_adj,indices_features):\n",
    "    print(\"R2 asjusted values:\")\n",
    "    \n",
    "    for i in range(len(list_r2_adj)):\n",
    "        print(list_r2_adj[i])\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Number of features chosen:\", len(indices_features))\n",
    "    print(\"\\n\")\n",
    "    print(\"Indices of features chosen: \", indices_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Least square model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No cross validation (NC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) NC: Use of the error before binarization (R2 with loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        #y = predict_labels(ws,X)   #***** NO USE OF PREDICTION\n",
    "        SSE = loss\n",
    "        SST = np.sum((Y- Y.mean())**2)\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) NC: Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X) \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) NC: Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = compute_loglikelihood_reg(y,X,w0)#np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = compute_loglikelihood_reg(y,X,ws) #np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike/loglike0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using cross validation (C) (TO BE DELETED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure that cross validation can be used we don't have to estimate any hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) C: R2 with likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0   # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "# parameters for cross validation\n",
    "arg_ls = dict()\n",
    "arg_ls['method'] = 'ls'\n",
    "arg_ls['loss'] = 'rmse'\n",
    "arg_ls['k_fold'] = 10\n",
    "\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        # CROSS-VALIDATION\n",
    "        \n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation(Y, X, arg_ls)\n",
    "        \n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used only lambda as hyperparameter because I am not building a polynomial model. But we can add different features transformation (square or log or power), so maybe we can test the polynomial after the best features are selected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## No cross validation (NC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set lambda\n",
    "lambda_ = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) (NC) Using the loss from ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # start with lambda = 0  \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        SSE = loss\n",
    "        SST = np.sum((Y- Y.mean())**2)   # it has no sense\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) (NC) Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [-1,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) (NC) Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # lambda set to 0 \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = compute_loglikelihood_reg(y,X,w0) #np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = compute_loglikelihood_reg(y,X,ws) #np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With cross validation (C) (it's needed in order to choose the best hyperparameter lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) (C) Using the loss of ridge regression as the error of R2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VEDIAMO CHE SUCCEDE QUA:\n",
      " SSE: 56311.660444  SST: 56311.660444  R2 5.22003232182e-14 R2_adj:  5.22003232182e-14\n",
      "R2adj_0=  5.22003232182e-14\n",
      "tested lambda_:  [  1.00000000e-10   1.66810054e-09   2.78255940e-08   4.64158883e-07\n",
      "   7.74263683e-06   1.29154967e-04   2.15443469e-03   3.59381366e-02\n",
      "   5.99484250e-01   1.00000000e+01] \n",
      "\n",
      "------------------------------------------ cross validation with lambda_ =  1e-10\n",
      "------------------------------------------ cross validation with lambda_ =  1.6681005372e-09\n",
      "------------------------------------------ cross validation with lambda_ =  2.78255940221e-08\n",
      "------------------------------------------ cross validation with lambda_ =  4.64158883361e-07\n",
      "------------------------------------------ cross validation with lambda_ =  7.74263682681e-06\n",
      "------------------------------------------ cross validation with lambda_ =  0.000129154966501\n",
      "------------------------------------------ cross validation with lambda_ =  0.00215443469003\n",
      "------------------------------------------ cross validation with lambda_ =  0.035938136638\n",
      "------------------------------------------ cross validation with lambda_ =  0.599484250319\n",
      "------------------------------------------ cross validation with lambda_ =  10.0\n",
      "Optimal lambda:  1e-10\n",
      "Associated testing loss:  0.4745831591 \n",
      "\n",
      "SSE: 0.4745831591  SST: 56311.660444  R2 0.999991572204 R2_adj:  [0.99999157217077761]\n",
      "tested lambda_:  [  1.00000000e-10   1.66810054e-09   2.78255940e-08   4.64158883e-07\n",
      "   7.74263683e-06   1.29154967e-04   2.15443469e-03   3.59381366e-02\n",
      "   5.99484250e-01   1.00000000e+01] \n",
      "\n",
      "------------------------------------------ cross validation with lambda_ =  1e-10\n",
      "------------------------------------------ cross validation with lambda_ =  1.6681005372e-09\n",
      "------------------------------------------ cross validation with lambda_ =  2.78255940221e-08\n",
      "------------------------------------------ cross validation with lambda_ =  4.64158883361e-07\n",
      "------------------------------------------ cross validation with lambda_ =  7.74263682681e-06\n",
      "------------------------------------------ cross validation with lambda_ =  0.000129154966501\n",
      "------------------------------------------ cross validation with lambda_ =  0.00215443469003\n",
      "------------------------------------------ cross validation with lambda_ =  0.035938136638\n",
      "------------------------------------------ cross validation with lambda_ =  0.599484250319\n",
      "------------------------------------------ cross validation with lambda_ =  10.0\n",
      "Optimal lambda:  1e-10\n",
      "Associated testing loss:  0.44433394278 \n",
      "\n",
      "SSE: 0.44433394278  SST: 56311.660444  R2 0.999992109379 R2_adj:  [0.99999157217077761, 0.99999210934792848]\n",
      "tested lambda_:  [  1.00000000e-10   1.66810054e-09   2.78255940e-08   4.64158883e-07\n",
      "   7.74263683e-06   1.29154967e-04   2.15443469e-03   3.59381366e-02\n",
      "   5.99484250e-01   1.00000000e+01] \n",
      "\n",
      "------------------------------------------ cross validation with lambda_ =  1e-10\n",
      "------------------------------------------ cross validation with lambda_ =  1.6681005372e-09\n",
      "------------------------------------------ cross validation with lambda_ =  2.78255940221e-08\n",
      "------------------------------------------ cross validation with lambda_ =  4.64158883361e-07\n",
      "------------------------------------------ cross validation with lambda_ =  7.74263682681e-06\n",
      "------------------------------------------ cross validation with lambda_ =  0.000129154966501\n",
      "------------------------------------------ cross validation with lambda_ =  0.00215443469003\n",
      "------------------------------------------ cross validation with lambda_ =  0.035938136638\n",
      "------------------------------------------ cross validation with lambda_ =  0.599484250319\n",
      "------------------------------------------ cross validation with lambda_ =  10.0\n",
      "Optimal lambda:  1e-10\n",
      "Associated testing loss:  0.474557368863 \n",
      "\n",
      "SSE: 0.474557368863  SST: 56311.660444  R2 0.999991572662 R2_adj:  [0.99999157217077761, 0.99999210934792848, 0.99999157262877059]\n",
      "tested lambda_:  [  1.00000000e-10   1.66810054e-09   2.78255940e-08   4.64158883e-07\n",
      "   7.74263683e-06   1.29154967e-04   2.15443469e-03   3.59381366e-02\n",
      "   5.99484250e-01   1.00000000e+01] \n",
      "\n",
      "------------------------------------------ cross validation with lambda_ =  1e-10\n",
      "------------------------------------------ cross validation with lambda_ =  1.6681005372e-09\n",
      "------------------------------------------ cross validation with lambda_ =  2.78255940221e-08\n",
      "------------------------------------------ cross validation with lambda_ =  4.64158883361e-07\n",
      "------------------------------------------ cross validation with lambda_ =  7.74263682681e-06\n",
      "------------------------------------------ cross validation with lambda_ =  0.000129154966501\n",
      "------------------------------------------ cross validation with lambda_ =  0.00215443469003\n",
      "------------------------------------------ cross validation with lambda_ =  0.035938136638\n",
      "------------------------------------------ cross validation with lambda_ =  0.599484250319\n",
      "------------------------------------------ cross validation with lambda_ =  10.0\n",
      "Optimal lambda:  1e-10\n",
      "Associated testing loss:  0.46572516084 \n",
      "\n",
      "SSE: 0.46572516084  SST: 56311.660444  R2 0.999991729508 R2_adj:  [0.99999157217077761, 0.99999210934792848, 0.99999157262877059, 0.99999172947449821]\n",
      "tested lambda_:  [  1.00000000e-10   1.66810054e-09   2.78255940e-08   4.64158883e-07\n",
      "   7.74263683e-06   1.29154967e-04   2.15443469e-03   3.59381366e-02\n",
      "   5.99484250e-01   1.00000000e+01] \n",
      "\n",
      "------------------------------------------ cross validation with lambda_ =  1e-10\n",
      "------------------------------------------ cross validation with lambda_ =  1.6681005372e-09\n",
      "------------------------------------------ cross validation with lambda_ =  2.78255940221e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b932c4523284>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m##########################################CROSS VALIDATION##########################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize_lambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_rr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m####################################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\utente\\Documents\\GitHub\\Project1_ML\\code\\COMMON\\optimize_hyperparams.py\u001b[0m in \u001b[0;36moptimize_lambda\u001b[1;34m(y, x, lambda_min, lambda_max, lambda_steps, args)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# cross validation with lambda_tmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mw_tr_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_tr_tot_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_te_tot_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# store mean losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\utente\\Documents\\GitHub\\Project1_ML\\code\\COMMON\\cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, X, args, debug_mode)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;31m# k'th train and test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mw_tr_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_tr_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_te_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# store weights and losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\utente\\Documents\\GitHub\\Project1_ML\\code\\COMMON\\cross_validation.py\u001b[0m in \u001b[0;36mcross_validation_k\u001b[1;34m(y, X, k_indices, k, args)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mX_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0my_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mX_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0my_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\utente\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munion1d\u001b[1;34m(ar1, ar2)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \"\"\"\n\u001b[1;32m--> 614\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msetdiff1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\utente\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid axis kwarg specified for unique'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\utente\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[0mflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # start with lambda = 0  \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss*2*n\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "print(\"VEDIAMO CHE SUCCEDE QUA:\\n SSE:\", sse, \" SST:\", sst, \" R2\", R2, \"R2_adj: \", R2adj_0)\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "print(\"R2adj_0= \", R2adj_0)\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "# ridge regression parameters\n",
    "arg_rr = dict()\n",
    "arg_rr['method'] = 'rr'\n",
    "arg_rr['loss'] = 'rmse'\n",
    "arg_rr['degree'] = 1\n",
    "arg_rr['k_fold'] = 10\n",
    "\n",
    "# optimization parameters\n",
    "lambda_min = -10 \n",
    "lambda_max = 1\n",
    "lambda_steps = 10\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        ##########################################CROSS VALIDATION##########################################\n",
    "        \n",
    "        ws, loss_tr, loss, lambda_opt = optimize_lambda(Y, X, lambda_min, lambda_max, lambda_steps, arg_rr)\n",
    "        \n",
    "        ####################################################################################################\n",
    "        \n",
    "        #ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        #loss = np.min(loss_te_tot)\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        SSE = loss\n",
    "        SST = np.sum((Y- Y.mean())**2)\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "        print(\"SSE:\", SSE, \" SST:\", SST, \" R2\", R2, \"R2_adj: \", R2_adj)\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2adj_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) (C) Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_rr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [-1,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) (C) Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # lambda set to 0 \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_rr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No cross validation (hyperparameter fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "max_iters = 100\n",
    "threshold = 1e-8\n",
    "gamma = 0\n",
    "method = 'gd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used only this method because it's the most reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "initial_w = np.zeros(X.shape[1])\n",
    "\n",
    "w0, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "y = predict_labels(w0[-1], X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        initial_w = np.zeros(X.shape[1])\n",
    "        ws, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "        y = predict_labels(ws[-1], X)\n",
    "        loglike = compute_loglikelihood_reg(y, X, ws)\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With cross correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_lr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
