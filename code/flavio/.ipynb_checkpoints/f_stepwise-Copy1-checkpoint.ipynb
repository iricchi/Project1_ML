{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features selection with STEPWISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "my_path = r'C:\\Users\\utente\\Documents\\GitHub\\Project1_ML'\n",
    "sys.path.insert(0,my_path + r'\\code\\COMMON')\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import load_csv_data, predict_labels \n",
    "from implementations import *\n",
    "from outliers import handle_outliers\n",
    "from labels import idx_2labels\n",
    "from standard import standardize\n",
    "from costs import compute_loglikelihood_reg\n",
    "from optimize_hyperparams import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and outliers management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(my_path + r'/data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-999 are replaced by the mean value of the feature\n"
     ]
    }
   ],
   "source": [
    "input_data, Y = handle_outliers(input_data,yb,-999,'mean') # substiution with mean because the standardization\n",
    "                                                           #can be affected, otherwise we should delete the whole row\n",
    "ind_back, ind_sig = idx_2labels(Y, [-1,1])\n",
    "Y[ind_back] = 0\n",
    "\n",
    "input_data, mean_X, std_X = standardize(input_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subdived the X features space in single features\n",
    "all_features = np.genfromtxt(my_path + r'/data/train.csv', delimiter=\",\", dtype=str, max_rows = 1)[2:]\n",
    "# converting array in list in order to simplify the adding of features\n",
    "all_features = list(all_features)\n",
    "\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R^2  as stopping criteria : 1) R2 with error , 2) R2 of Tjur or 3) R2 of McFadden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only way to use the stepwise is using R2 of Tjur or McFadden because of the binary values of the indipendent variable, but the error was also used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def results_r2_stepwise(list_r2_adj,indices_features):\n",
    "    print(\"R2 asjusted values:\")\n",
    "    \n",
    "    for i in range(len(list_r2_adj)):\n",
    "        print(list_r2_adj[i])\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Number of features chosen:\", len(indices_features))\n",
    "    print(\"\\n\")\n",
    "    print(\"Indices of features chosen: \", indices_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Least square model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No cross validation (NC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) NC: Use of the error before binarization (R2 with loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltaeta_jet_jet (index : 4 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_pt (index : 16 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_ratio_lep_tau (index : 10 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_lep_eta_centrality (index : 12 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_pt (index : 23 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_pt (index : 26 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_num (index : 22 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met_sumet (index : 21 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_MMC (index : 0 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_prodeta_jet_jet (index : 6 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_h (index : 3 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss*2*n\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        #y = predict_labels(ws,X)   #***** NO USE OF PREDICTION\n",
    "        SSE = loss*2*n\n",
    "        SST = np.sum((Y- Y.mean())**2)\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.123498102139\n",
      "0.158127051218\n",
      "0.183285452779\n",
      "0.19737862558\n",
      "0.204845050856\n",
      "0.212458380282\n",
      "0.221835080472\n",
      "0.22963272572\n",
      "0.234367607585\n",
      "0.237448089064\n",
      "0.239839917382\n",
      "0.241052424021\n",
      "0.242204099894\n",
      "0.243490389003\n",
      "0.243684846279\n",
      "0.244154585661\n",
      "0.244186288257\n",
      "0.244191908959\n",
      "0.244195224799\n",
      "0.24419603504\n",
      "0.244196344677\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 21\n",
      "\n",
      "\n",
      "Indices of features chosen:  [1, 13, 4, 11, 7, 2, 16, 10, 19, 12, 23, 8, 5, 26, 22, 21, 0, 18, 6, 3, 28]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) NC: Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X) \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) NC: Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = compute_loglikelihood_reg(y,X,w0)#np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws , loss = least_squares(Y,X)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = compute_loglikelihood_reg(y,X,ws) #np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike/loglike0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cross validation (C) (TO BE DELETED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure that cross validation can be used we don't have to estimate any hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) (C): R2 with likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = least_squares(Y,X)  # The loss cannot be used as a measure for the feature selection because it's a \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0   # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "# parameters for cross validation\n",
    "arg_ls = dict()\n",
    "arg_ls['method'] = 'ls'\n",
    "arg_ls['loss'] = 'rmse'\n",
    "arg_ls['k_fold'] = 10\n",
    "\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        # CROSS-VALIDATION\n",
    "        \n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation(Y, X, arg_ls)\n",
    "        \n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used only lambda as hyperparameter because I am not building a polynomial model. But we can add different features transformation (square or log or power), so maybe we can test the polynomial after the best features are selected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## No cross validation (NC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set lambda\n",
    "lambda_ = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) (NC) Using the loss from ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_pt (index : 13 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltaeta_jet_jet (index : 4 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_deltar_tau_lep (index : 7 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_lep_eta_centrality (index : 12 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_met (index : 19 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_jet_jet (index : 5 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_pt (index : 26 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_tot (index : 8 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_pt_h (index : 3 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_num (index : 22 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_tau_phi (index : 15 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # start with lambda = 0  \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss*2*n\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        SSE = loss*2*n\n",
    "        SST = np.sum((Y- Y.mean())**2)   # it has no sense\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.0590175831678\n",
      "0.0919564321754\n",
      "0.115305280064\n",
      "0.131172460715\n",
      "0.137081038671\n",
      "0.14163931635\n",
      "0.143708108081\n",
      "0.145592367705\n",
      "0.146688011854\n",
      "0.148345611287\n",
      "0.148987515541\n",
      "0.149696003179\n",
      "0.14984311946\n",
      "0.149851916892\n",
      "0.149853409866\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 15\n",
      "\n",
      "\n",
      "Indices of features chosen:  [1, 13, 4, 11, 7, 12, 19, 2, 5, 26, 8, 3, 22, 18, 15]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) (NC) Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [-1,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) (NC) Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_transverse_met_lep (index : 1 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_met_phi_centrality (index : 11 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_sum_pt (index : 9 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  DER_mass_vis (index : 2 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_lep_phi (index : 18 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_subleading_phi (index : 28 )\n",
      "-------------------------------------------------\n",
      "Feature chosen:  PRI_jet_leading_eta (index : 24 )\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # lambda set to 0 \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = compute_loglikelihood_reg(y,X,w0) #np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        ws, loss = ridge_regression(Y,X,lambda_)\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = compute_loglikelihood_reg(y,X,ws) #np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 asjusted values:\n",
      "0.127200013384\n",
      "0.161949192879\n",
      "0.162775051476\n",
      "0.163218910102\n",
      "0.163296601788\n",
      "0.1633260918\n",
      "0.163331743587\n",
      "-------------------------------------------------------\n",
      "Number of features chosen: 7\n",
      "\n",
      "\n",
      "Indices of features chosen:  [1, 11, 9, 2, 18, 28, 24]\n"
     ]
    }
   ],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With cross validation (C) (it's needed in order to choose the best hyperparameter lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) (C) Using the loss of ridge regression as the error of R2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2adj_0=  5.22003232182e-14\n",
      "SSE: 237291.57955  SST: 56311.660444  R2 3.21389775544 R2_adj:  [3.2139066111045418]\n",
      "SSE: 222166.97139  SST: 56311.660444  R2 2.94531025437 R2_adj:  [3.2139066111045418, 2.9453180356717179]\n",
      "SSE: 237278.684431  SST: 56311.660444  R2 3.21366875991 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186]\n",
      "SSE: 232862.58042  SST: 56311.660444  R2 3.13524620982 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942]\n",
      "SSE: 233286.64172  SST: 56311.660444  R2 3.14277682244 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813]\n",
      "SSE: 233563.179252  SST: 56311.660444  R2 3.14768766203 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343]\n",
      "SSE: 234083.688294  SST: 56311.660444  R2 3.15693102368 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343, 3.1569396514684209]\n",
      "SSE: 237284.407098  SST: 56311.660444  R2 3.21377038481 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343, 3.1569396514684209, 3.2137792399633129]\n",
      "SSE: 237275.312461  SST: 56311.660444  R2 3.21360887941 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343, 3.1569396514684209, 3.2137792399633129, 3.2136177339159024]\n",
      "SSE: 234499.80327  SST: 56311.660444  R2 3.16432052299 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343, 3.1569396514684209, 3.2137792399633129, 3.2136177339159024, 3.1643291803372549]\n",
      "SSE: 232727.90769  SST: 56311.660444  R2 3.13285464955 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343, 3.1569396514684209, 3.2137792399633129, 3.2136177339159024, 3.1643291803372549, 3.1328631810319014]\n",
      "SSE: 228371.559773  SST: 56311.660444  R2 3.05549326679 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343, 3.1569396514684209, 3.2137792399633129, 3.2136177339159024, 3.1643291803372549, 3.1328631810319014, 3.0555014888334062]\n",
      "SSE: 233756.930616  SST: 56311.660444  R2 3.15112835908 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343, 3.1569396514684209, 3.2137792399633129, 3.2136177339159024, 3.1643291803372549, 3.1328631810319014, 3.0555014888334062, 3.1511369636591851]\n",
      "SSE: 230643.314117  SST: 56311.660444  R2 3.0958357878 R2_adj:  [3.2139066111045418, 2.9453180356717179, 3.2136776146527186, 3.1352547508779942, 3.1427853936117813, 3.1476962528492343, 3.1569396514684209, 3.2137792399633129, 3.2136177339159024, 3.1643291803372549, 3.1328631810319014, 3.0555014888334062, 3.1511369636591851, 3.0958441712067239]\n"
     ]
    }
   ],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # start with lambda = 0  \n",
    "y = predict_labels(w0, X)\n",
    "\n",
    "sse = loss*2*n\n",
    "sst = np.sum((Y - Y.mean())**2)  #lack of information\n",
    "R2 = np.abs((sst-sse)/sst)\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "print(\"R2adj_0= \", R2adj_0)\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "# ridge regression parameters\n",
    "arg_rr = dict()\n",
    "arg_rr['method'] = 'rr'\n",
    "arg_rr['loss'] = 'rmse'\n",
    "arg_rr['degree'] = 1\n",
    "arg_rr['k_fold'] = 10\n",
    "\n",
    "# optimization parameters\n",
    "lambda_min = -10 \n",
    "lambda_max = 1\n",
    "lambda_steps = 10\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        ##########################################CROSS VALIDATION##########################################\n",
    "        \n",
    "        ws, loss_tr, loss, lambda_opt = optimize_lambda(Y, X, lambda_min, lambda_max, lambda_steps, arg_rr)\n",
    "        \n",
    "        ####################################################################################################\n",
    "        \n",
    "        #ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        #loss = np.min(loss_te_tot)\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        SSE = loss*2*n\n",
    "        SST = np.sum((Y- Y.mean())**2)\n",
    "        R2 = np.abs((SST-SSE)/SST)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "        print(\"SSE:\", SSE, \" SST:\", SST, \" R2\", R2, \"R2_adj: \", R2_adj)\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R2adj_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) (C) Use of the probability of the 2 events (R2 Tjur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  \n",
    "y = predict_labels(w0, X)\n",
    "ind_back, ind_sig = idx_2labels(y, [0,1])\n",
    "\n",
    "y_ = X.dot(w0)\n",
    "R2 = 0\n",
    "R2adj_0 = R2 - (k/(n-k-1)*(1-R2))\n",
    "\n",
    "#fix the R2adj_max\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_rr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)          \n",
    "        ind_back, ind_sig = idx_2labels(y, [-1,1])\n",
    "        \n",
    "        if len(ind_sig) == 0 or len(ind_back) ==0:\n",
    "            print('No signal detected')\n",
    "            R2_adj.append(0)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            y_ = X.dot(ws)\n",
    "\n",
    "            R2 = np.abs((np.mean(y_[ind_sig]) - np.mean(y_[ind_back])))\n",
    "            R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "            \n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "\n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "\n",
    "        #idx_features.append(np.where(all_candidates[:,ind_max] == input_data))\n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "\n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "\n",
    "        del(X)\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) (C) Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = ridge_regression(Y,X,0)  # lambda set to 0 \n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_rr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No cross validation (hyperparameter fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "max_iters = 100\n",
    "threshold = 1e-8\n",
    "gamma = 0\n",
    "method = 'gd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used only this method because it's the most reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "initial_w = np.zeros(X.shape[1])\n",
    "\n",
    "w0, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "y = predict_labels(w0[-1], X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        \n",
    "        initial_w = np.zeros(X.shape[1])\n",
    "        ws, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "        \n",
    "        k = len(ws) -1 # k is the number of regressor I use -> -1 because I don't consider the offset\n",
    "        \n",
    "        y = predict_labels(ws[-1], X)\n",
    "        loglike = compute_loglikelihood_reg(y, X, ws)\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With cross correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of the likelihood (McFadden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realloc FEATURES\n",
    "features = []\n",
    "for i in range(len(all_features)):\n",
    "    features.append((i,all_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of STEP-WISE algorithm \n",
    "all_candidates = input_data\n",
    "\n",
    "n = all_candidates.shape[0] #needed for the R^2 adjusted\n",
    "num = all_candidates.shape[1]\n",
    "H = np.ones((n,1)) #offset\n",
    "\n",
    "#Initialization only with offsets (lack of info)\n",
    "X = H\n",
    "k = 0 #needed for the R^2 adjusted\n",
    "\n",
    "w0, loss = logistic_regression(Y,X, initial_w, max_iters, gamma, method)\n",
    "y = predict_labels(w0, X)\n",
    "loglike0 = np.sum(np.log(1+np.exp(X.dot(w0))) - y*(X.dot(w0)))\n",
    "\n",
    "R2 = 0        # For the definition of McFadden 1-1 = 0\n",
    "R2adj_0 = 0\n",
    "\n",
    "#fix the R2adj_max\n",
    "\n",
    "R2adj_max = R2adj_0\n",
    "ind_max = 0  # this index will show us which is the best feature chosen\n",
    "del(X)\n",
    "idx_features = []\n",
    "best_R2adj = []\n",
    "\n",
    "for j in range(num):\n",
    "    R2_adj = []\n",
    "    for i in range(all_candidates.shape[1]):\n",
    "        \n",
    "        X = np.concatenate((H,all_candidates[:,i].reshape(n,1)), axis=1)\n",
    "        #CROSS VALIDATION\n",
    "        w_tr_tot, loss_tr_tot, loss_te_tot = cross_validation_lr(Y,X)\n",
    "        ws = w_tr_tot[np.argmin(loss_te_tot)]\n",
    "        \n",
    "        y = predict_labels(ws,X)   \n",
    "        \n",
    "        loglike = np.sum(np.log(1+np.exp(X.dot(ws))) - y*(X.dot(ws)))\n",
    "        \n",
    "        R2 = 1-(loglike/loglike0)\n",
    "        R2_adj.append(R2 - (k/(n-k-1)*(1-R2)))\n",
    "        \n",
    "    R2adj_chosen = np.max(R2_adj)\n",
    "    best_R2adj.append(R2adj_chosen)\n",
    "    idx_chosen = np.argmax(R2_adj)\n",
    "    \n",
    "    if R2adj_chosen > R2adj_max:\n",
    "        R2adj_max = R2adj_chosen\n",
    "        ind_max = idx_chosen\n",
    "        \n",
    "        H = np.concatenate((H, all_candidates[:,ind_max].reshape(n,1)), axis = 1)\n",
    "        \n",
    "        all_candidates = np.delete(all_candidates,ind_max,1)\n",
    "        print('-------------------------------------------------')\n",
    "        print('Feature chosen: ', features[ind_max][1], '(index :', features[ind_max][0], ')')\n",
    "        idx_features.append(features[ind_max][0])\n",
    "        del(features[ind_max])\n",
    "        \n",
    "        del(X)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_r2_stepwise(best_R2adj[:len(best_R2adj)-1], idx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
